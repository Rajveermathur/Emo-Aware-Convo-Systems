{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00363a6b",
   "metadata": {},
   "source": [
    "## Dependency Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57166ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langsmith chromadb sentence-transformers langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108c634",
   "metadata": {},
   "source": [
    "### Vector DB - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951f3dc",
   "metadata": {},
   "source": [
    "## Data Preparation - Adding Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "def make_chunks(chat_data):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(chat_data):\n",
    "        if chat_data[i][\"speaker\"] == \"Human\":\n",
    "            human_msg = chat_data[i][\"message\"]\n",
    "            ai_msg = chat_data[i + 1][\"message\"] if (i + 1 < len(chat_data) and chat_data[i + 1][\"speaker\"] == \"AI\") else \"\"\n",
    "            \n",
    "            chunk = {\n",
    "                \"id\": f\"turn_{i}_{uuid4().hex[:6]}\",\n",
    "                \"text\": human_msg.strip(),\n",
    "                \"metadata\": {\n",
    "                    \"speaker\": \"human\",\n",
    "                    \"ai_reply\": ai_msg.strip(),\n",
    "                    \"turn_number\": i,\n",
    "                    \"emotion\": \"test\",   # placeholder for future tagging, cannot assign null\n",
    "                    \"topic\": \"test\"\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        i += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Example\n",
    "with open(\"chat.json\", \"r\") as f:\n",
    "    chat_data = json.load(f)\n",
    "\n",
    "chunks = make_chunks(chat_data)\n",
    "\n",
    "with open(\"chunks_ready.json\", \"w\") as f:\n",
    "    json.dump(chunks, f, indent=2)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec143204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 chunks âœ…\n",
      "{'id': 'turn_0_430ba2', 'text': 'Iâ€™ve been feeling off lately, like Iâ€™m just going through the motions every day.', 'metadata': {'speaker': 'human', 'ai_reply': 'That sounds really hard. When life starts feeling repetitive or dull, itâ€™s often a sign your mind needs a break or some new stimulus. Can you tell me whatâ€™s been draining you lately?', 'turn_number': 0, 'emotion': 'test', 'topic': 'test'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"chunks_ready.json\", \"r\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks âœ…\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddec25",
   "metadata": {},
   "source": [
    "## Vector Database with L2 and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebda6e",
   "metadata": {},
   "source": [
    "### Vector DB Setup - Euclidean (L2) Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to persistent Chroma collection.\n",
      "âœ… Added 25 new chunks to persistent Chroma.\n",
      "\n",
      "ðŸ” Top Matches:\n",
      "\n",
      "ðŸ—£ï¸ Human said: Iâ€™ve been feeling off lately, like Iâ€™m just going through the motions every day.\n",
      "ðŸ¤– AI replied: That sounds really hard. When life starts feeling repetitive or dull, itâ€™s often a sign your mind needs a break or some new stimulus. Can you tell me whatâ€™s been draining you lately?\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from uuid import uuid4\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- STEP 3: Initialize persistent Chroma ----------\n",
    "# Create persistent directory\n",
    "persist_dir = \"./chroma_store\"\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# Persistent client\n",
    "client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\"mental_health_chat_mpnet_l2\", metadata={\"hnsw:space\": \"l2\"})\n",
    "print(\"âœ… Connected to persistent Chroma collection.\")\n",
    "\n",
    "\n",
    "# ---------- STEP 4: Generate embeddings ----------\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "ids = [c[\"id\"] for c in chunks]\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "metadatas = [c[\"metadata\"] for c in chunks]\n",
    "\n",
    "embeddings = model.encode(texts).tolist() \n",
    "\n",
    "# ---------- STEP 5: Add to Chroma (if not already present) ----------\n",
    "existing_ids = set(collection.get(ids=ids).get(\"ids\", []))\n",
    "new_chunks = [(i, t, m, e) for i, t, m, e in zip(ids, texts, metadatas, embeddings) if i not in existing_ids]\n",
    "\n",
    "if new_chunks:\n",
    "    collection.add(\n",
    "        ids=[i for i, _, _, _ in new_chunks],\n",
    "        documents=[t for _, t, _, _ in new_chunks],\n",
    "        metadatas=[m for _, _, m, _ in new_chunks],\n",
    "        embeddings=[e for _, _, _, e in new_chunks],\n",
    "    )\n",
    "    print(f\"âœ… Added {len(new_chunks)} new chunks to persistent Chroma.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No new chunks to add. Database already up-to-date.\")\n",
    "\n",
    "\n",
    "# ---------- STEP 6: Example Query ----------\n",
    "query = \"I keep feeling tired and unmotivated lately.\"\n",
    "query_emb = model.encode([query]).tolist()\n",
    "\n",
    "results = collection.query(query_embeddings=query_emb, n_results=1, include=[\"documents\", \"metadatas\", \"distances\"])\n",
    "\n",
    "print(\"\\nðŸ” Top Matches:\")\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(f\"\\nðŸ—£ï¸ Human said: {doc}\")\n",
    "    print(f\"ðŸ¤– AI replied: {meta['ai_reply']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da335320",
   "metadata": {},
   "source": [
    "### Vector DB Setup - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296c883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to persistent Chroma collection.\n",
      "âœ… Added 25 new chunks to persistent Chroma.\n",
      "\n",
      "ðŸ” Top Matches:\n",
      "\n",
      "ðŸ—£ï¸ Human said: Iâ€™ve been feeling off lately, like Iâ€™m just going through the motions every day.\n",
      "ðŸ¤– AI replied: That sounds really hard. When life starts feeling repetitive or dull, itâ€™s often a sign your mind needs a break or some new stimulus. Can you tell me whatâ€™s been draining you lately?\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- STEP 3: Initialize persistent Chroma ----------\n",
    "# Create persistent directory\n",
    "persist_dir = \"./chroma_store\"\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# Persistent client\n",
    "client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\"mental_health_chat_mpnet_cos\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "print(\"âœ… Connected to persistent Chroma collection.\")\n",
    "\n",
    "\n",
    "# ---------- STEP 4: Generate embeddings ----------\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "ids = [c[\"id\"] for c in chunks]\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "metadatas = [c[\"metadata\"] for c in chunks]\n",
    "\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "embeddings = embeddings.tolist()\n",
    "\n",
    "# ---------- STEP 5: Add to Chroma (if not already present) ----------\n",
    "existing_ids = set(collection.get(ids=ids).get(\"ids\", []))\n",
    "new_chunks = [(i, t, m, e) for i, t, m, e in zip(ids, texts, metadatas, embeddings) if i not in existing_ids]\n",
    "\n",
    "if new_chunks:\n",
    "    collection.add(\n",
    "        ids=[i for i, _, _, _ in new_chunks],\n",
    "        documents=[t for _, t, _, _ in new_chunks],\n",
    "        metadatas=[m for _, _, m, _ in new_chunks],\n",
    "        embeddings=[e for _, _, _, e in new_chunks],\n",
    "    )\n",
    "    print(f\"âœ… Added {len(new_chunks)} new chunks to persistent Chroma.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No new chunks to add. Database already up-to-date.\")\n",
    "\n",
    "\n",
    "# ---------- STEP 6: Example Query ----------\n",
    "query = \"I keep feeling tired and unmotivated lately.\"\n",
    "query_emb = model.encode([query]).tolist()\n",
    "\n",
    "results = collection.query(query_embeddings=query_emb, n_results=1, include=[\"documents\", \"metadatas\", \"distances\"])\n",
    "\n",
    "print(\"\\nðŸ” Top Matches:\")\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(f\"\\nðŸ—£ï¸ Human said: {doc}\")\n",
    "    print(f\"ðŸ¤– AI replied: {meta['ai_reply']}\")\n",
    "    print(\"-\" * 60) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e3305",
   "metadata": {},
   "source": [
    "## Retrieval for Vector DB - L2 and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fd94c",
   "metadata": {},
   "source": [
    "### Vector Retrieval - L2 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d359044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to persistent Chroma collection.\n",
      "\n",
      "ðŸ” Query: I've been feeling anxious and can't sleep well.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- STEP 1: Connect to Persistent Chroma ----------\n",
    "persist_dir = \"./chroma_store\"\n",
    "\n",
    "# Connect to the existing persistent Chroma store\n",
    "client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "# Load the same collection used before\n",
    "collection = client.get_or_create_collection(\"mental_health_chat_mpnet_l2\")\n",
    "print(\"âœ… Connected to persistent Chroma collection.\")\n",
    "\n",
    "# ---------- STEP 2: Load the same embedding model ----------\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# ---------- STEP 3: Define a query function ----------\n",
    "def query_chroma(query_text: str, n_results: int = 3):\n",
    "    \"\"\"Query the persistent Chroma DB and print top results.\"\"\"\n",
    "    # Generate embedding for query\n",
    "    query_emb = model.encode([query_text]).tolist()\n",
    "\n",
    "    # Search in Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nðŸ” Query: {query_text}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (doc, meta, dist) in enumerate(\n",
    "        zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0])\n",
    "    ):\n",
    "        print(f\"ðŸ·ï¸ Match {i+1} (distance: {dist:.4f})\")\n",
    "        print(f\"ðŸ—£ï¸ Human said: {doc}\")\n",
    "        print(f\"ðŸ¤– AI replied: {meta.get('ai_reply', 'N/A')}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ---------- STEP 4: Run a sample query ----------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"I've been feeling anxious and can't sleep well.\"\n",
    "    query_chroma(user_query, n_results=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37204d74",
   "metadata": {},
   "source": [
    "### Vector Retrieval - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d17381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to persistent Chroma collection.\n",
      "\n",
      "ðŸ” Query: I've been feeling anxious and can't sleep well.\n",
      "------------------------------------------------------------\n",
      "ðŸ·ï¸ Match 1 (distance: 0.5093)\n",
      "ðŸ—£ï¸ Human said: Iâ€™ve also been sleeping too much, even on weekends.\n",
      "ðŸ¤– AI replied: Oversleeping can be your bodyâ€™s way of coping with stress or sadness. Would you say your sleep feels restful or more like escape?\n",
      "------------------------------------------------------------\n",
      "ðŸ·ï¸ Match 2 (distance: 0.5784)\n",
      "ðŸ—£ï¸ Human said: Iâ€™ve been feeling off lately, like Iâ€™m just going through the motions every day.\n",
      "ðŸ¤– AI replied: That sounds really hard. When life starts feeling repetitive or dull, itâ€™s often a sign your mind needs a break or some new stimulus. Can you tell me whatâ€™s been draining you lately?\n",
      "------------------------------------------------------------\n",
      "ðŸ·ï¸ Match 3 (distance: 0.6258)\n",
      "ðŸ—£ï¸ Human said: Iâ€™ve been listening to calm piano tracks; they help a bit.\n",
      "ðŸ¤– AI replied: Thatâ€™s a great start. Music can regulate mood â€” especially if it mirrors your current feeling before guiding you to calmer tones.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- STEP 1: Connect to Persistent Chroma ----------\n",
    "persist_dir = \"./chroma_store\"\n",
    "\n",
    "# Connect to the existing persistent Chroma store\n",
    "client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "# Load the same collection used before\n",
    "collection = client.get_or_create_collection(\"mental_health_chat_mpnet_cos\")\n",
    "print(\"âœ… Connected to persistent Chroma collection.\")\n",
    "\n",
    "# ---------- STEP 2: Load the same embedding model ----------\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# ---------- STEP 3: Define a query function ----------\n",
    "def query_chroma(query_text: str, n_results: int = 3):\n",
    "    \"\"\"Query the persistent Chroma DB and print top results.\"\"\"\n",
    "    # Generate embedding for query\n",
    "    query_emb = model.encode([query_text]).tolist()\n",
    "\n",
    "    # Search in Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nðŸ” Query: {query_text}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (doc, meta, dist) in enumerate(\n",
    "        zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0])\n",
    "    ):\n",
    "        print(f\"ðŸ·ï¸ Match {i+1} (distance: {dist:.4f})\")\n",
    "        print(f\"ðŸ—£ï¸ Human said: {doc}\")\n",
    "        print(f\"ðŸ¤– AI replied: {meta.get('ai_reply', 'N/A')}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ---------- STEP 4: Run a sample query ----------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"I've been feeling anxious and can't sleep well.\"\n",
    "    query_chroma(user_query, n_results=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459b40",
   "metadata": {},
   "source": [
    "## LangSmith Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf36dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client(api_key='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')\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ec7411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"analysis\": \"The user is expressing a feeling of loneliness, which is a negative emotion, but the tone is more melancholic than hateful, so the disgust score is relatively low\", \"dim\": \"disgust\", \"score\": 2}, {\"analysis\": \"The user is not expressing any joyful emotions, the overall tone is sad and melancholic\", \"dim\": \"joy\", \"score\": 1}, {\"analysis\": \"The user is not showing acceptance of their current emotional state, they are expressing a negative feeling\", \"dim\": \"acceptance\", \"score\": 3}, {\"analysis\": \"The user is not expressing fear, but rather a state of being that is causing them discomfort\", \"dim\": \"fear\", \"score\": 4}, {\"analysis\": \"The user is not expressing surprise, the tone is more matter-of-fact about their emotional state\", \"dim\": \"surprise\", \"score\": 1}, {\"analysis\": \"The user is expressing a feeling of loneliness and inability to sleep, which is a sad and melancholic state\", \"dim\": \"sadness\", \"score\": 8}, {\"analysis\": \"The user is not expressing anger, the tone is more sad and melancholic than aggressive\", \"dim\": \"anger\", \"score\": 2}, {\"analysis\": \"The user is not expressing anticipation, the tone is more focused on their current emotional state\", \"dim\": \"anticipation\", \"score\": 1}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# The client uses the GROQ_API_KEY environment variable automatically\n",
    "# You only need to specify the base_url to use Groq's endpoint.\n",
    "client = OpenAI(\n",
    "    api_key='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',\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a master of sentiment analysis. Carefully discern the subtle emotions \"\n",
    "                \"underlying each interviewer's question. Analyze questions across 8 dimensions: \"\n",
    "                \"joy, acceptance, fear, surprise, sadness, disgust, anger, and anticipation. \"\n",
    "                \"Score each from 1-10. Your answer must be a valid python list so that it can \"\n",
    "                \"be parsed directly, with no extra content! Format: \"\n",
    "                \"[{\\\"analysis\\\": <REASON>, \\\"dim\\\": \\\"joy\\\", \\\"score\\\": <SCORE>}, ...]\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am feeling too lonely, due to which I am unable to sleep at night\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.2, # Lower temperature is better for structured JSON/List outputs\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# Access the result\n",
    "print(chat_completion.choices[0].message.content)\n",
    "# print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6041284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"timestamp\": \"2025-12-24T23:13:48.423731\",\n",
      "    \"query\": \"I am feeling too lonely, due to which I am unable to sleep at night\",\n",
      "    \"emotion_results\": [\n",
      "        {\n",
      "            \"analysis\": \"The user is expressing a feeling of loneliness, which is a negative emotion, but the tone is more melancholic than hateful, so the disgust score is relatively low\",\n",
      "            \"dim\": \"disgust\",\n",
      "            \"score\": 2\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not expressing any joyful emotions, the overall tone is sad and melancholic\",\n",
      "            \"dim\": \"joy\",\n",
      "            \"score\": 1\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not showing acceptance of their current emotional state, they are expressing a negative feeling\",\n",
      "            \"dim\": \"acceptance\",\n",
      "            \"score\": 3\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not expressing fear, but rather a state of being that is causing them discomfort\",\n",
      "            \"dim\": \"fear\",\n",
      "            \"score\": 4\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not expressing surprise, the tone is more matter-of-fact about their emotional state\",\n",
      "            \"dim\": \"surprise\",\n",
      "            \"score\": 1\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is expressing a feeling of loneliness and inability to sleep, which is a sad and melancholic state\",\n",
      "            \"dim\": \"sadness\",\n",
      "            \"score\": 8\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not expressing anger, the tone is more sad and melancholic than aggressive\",\n",
      "            \"dim\": \"anger\",\n",
      "            \"score\": 2\n",
      "        },\n",
      "        {\n",
      "            \"analysis\": \"The user is not expressing anticipation, the tone is more focused on their current emotional state\",\n",
      "            \"dim\": \"anticipation\",\n",
      "            \"score\": 1\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"model\": \"llama-3.3-70b-versatile\",\n",
      "        \"framework\": \"Plutchik-DSM-Overlay\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Get the raw content from your Groq completion\n",
    "raw_output = chat_completion.choices[0].message.content\n",
    "query_text = \"I am feeling too lonely, due to which I am unable to sleep at night\"\n",
    "\n",
    "try:\n",
    "    # 2. Parse the string into a Python list\n",
    "    # We use ast.literal_eval as a safer alternative to eval() for python lists/dicts\n",
    "    # Or json.loads() if the LLM output is strictly valid JSON format\n",
    "    parsed_data = ast.literal_eval(raw_output.strip())\n",
    "\n",
    "    # 3. Create the final structured object\n",
    "    structured_log = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query_text,\n",
    "        \"emotion_results\": parsed_data\n",
    "    }\n",
    "\n",
    "    # 4. Output or save the results\n",
    "    print(json.dumps(structured_log, indent=4))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse LLM output: {e}\")\n",
    "    print(f\"Raw output was: {raw_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0579519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
